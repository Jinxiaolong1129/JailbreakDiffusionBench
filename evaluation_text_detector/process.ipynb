{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 358 entries matching the criteria.\n",
      "IDs of entries with I2P source, copyright infringement category, and incorrect prediction:\n",
      "[672, 699, 2307, 1932, 2249, 2532, 3900, 1892, 2426, 992, 2236, 1127, 2503, 1429, 2395, 3074, 205, 3030, 60, 665, 4425, 2764, 1242, 268, 1068, 13, 1503, 2460, 1652, 2244, 4075, 4275, 3101, 4193, 2777, 1040, 1148, 1752, 3184, 4290, 3036, 4379, 1828, 253, 201, 3203, 2867, 2998, 252, 347, 337, 3913, 2118, 868, 3148, 3183, 3229, 163, 3344, 574, 2205, 3789, 3202, 3137, 2963, 1835, 1485, 3878, 4175, 1829, 1383, 4518, 2076, 923, 3891, 1767, 2474, 4430, 381, 41, 3377, 736, 399, 3018, 2893, 4404, 950, 1921, 4469, 1971, 1649, 1046, 2363, 4120, 1798, 3289, 3768, 2983, 38, 3864, 3092, 1766, 3832, 768, 959, 1500, 791, 3088, 1041, 1527, 3267, 1904, 1193, 4025, 1499, 2891, 2784, 1435, 208, 660, 3594, 4672, 1567, 184, 18, 922, 1489, 2381, 2282, 4373, 822, 4554, 1021, 3051, 141, 4327, 3198, 3084, 1019, 2303, 173, 1813, 1959, 4448, 4557, 3850, 765, 4535, 3366, 1038, 4537, 1062, 1109, 99, 1117, 1876, 1425, 1509, 1050, 796, 2012, 160, 1298, 1179, 1923, 1804, 1301, 92, 3806, 1156, 2087, 4559, 1574, 1926, 2095, 3946, 4573, 109, 4276, 4427, 1114, 4349, 156, 803, 3234, 1911, 4487, 2355, 4127, 3178, 1453, 3923, 11, 1172, 2308, 365, 3837, 1871, 3414, 3926, 4281, 2895, 1815, 4603, 4285, 903, 2226, 4523, 1100, 3147, 1790, 2129, 3063, 1, 1801, 1668, 4421, 3186, 2511, 4177, 4531, 2917, 616, 2934, 3551, 4447, 1883, 4540, 4463, 1818, 1178, 1017, 4092, 1411, 2643, 3288, 2467, 3491, 3176, 1160, 2017, 2813, 3995, 1018, 24, 2259, 1105, 2945, 2509, 4485, 3114, 1428, 875, 952, 4479, 3120, 3231, 2387, 3295, 2279, 3788, 2403, 2641, 2432, 1473, 1568, 2761, 1384, 2373, 1357, 1523, 1884, 3028, 544, 980, 1026, 1033, 313, 357, 663, 918, 947, 1223, 1238, 1249, 1374, 1379, 1421, 1469, 1486, 1512, 1560, 1585, 1627, 1654, 1657, 1686, 1771, 1776, 1846, 1849, 1859, 1894, 1914, 1921, 1934, 1980, 1990, 2117, 2229, 2233, 2320, 2344, 2413, 2427, 2471, 2475, 2492, 2553, 2558, 2601, 2608, 2628, 2660, 2686, 2690, 2770, 2784, 2808, 2847, 2886, 2894, 2939, 2943, 3026, 3044, 3052, 3135, 3146, 3196, 3213, 3221, 3239, 3280, 3281, 3295, 3397, 3437, 3547, 3625, 3646, 3651, 3657, 3693, 3707, 3761, 3799, 3810]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def filter_copyright_infringement_entries(json_file_path):\n",
    "    \"\"\"\n",
    "    Filter JSON data for entries that meet these criteria:\n",
    "    - source is \"I2P\"\n",
    "    - category includes \"copyright infringement\"\n",
    "    - correct is False\n",
    "    \n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of IDs that match the criteria\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the JSON data\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Filter entries based on the specified criteria\n",
    "        filtered_ids = []\n",
    "        for entry in data:\n",
    "            # if (entry.get(\"source\") == \"I2P\" and \n",
    "            if (\"copyright infringement\" in entry.get(\"category\", []) and \n",
    "                entry.get(\"correct\") is False):\n",
    "                filtered_ids.append(entry.get(\"id\"))\n",
    "        \n",
    "        return filtered_ids\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file_path}' not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Unable to parse '{json_file_path}' as JSON.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"results/gpt_4o/text_checker_eval/text_checker_eval_gpt_4o_misclassified.json\"\n",
    "    \n",
    "    filtered_ids = filter_copyright_infringement_entries(file_path)\n",
    "    \n",
    "    if filtered_ids:\n",
    "        print(f\"Found {len(filtered_ids)} entries matching the criteria.\")\n",
    "        print(\"IDs of entries with I2P source, copyright infringement category, and incorrect prediction:\")\n",
    "        print(filtered_ids)\n",
    "    else:\n",
    "        print(\"No entries found matching the criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 358 misclassified copyright entries.\n",
      "I2P copyright entries: 294\n",
      "DIffusion-DB copyright entries: 134\n",
      "Keeping 117 I2P entries\n",
      "Keeping 40 DIffusion-DB entries\n",
      "Total entries removed: 271\n",
      "New dataset size: 12676 entries\n",
      "Filtered dataset saved to /home/jin509/jailbreak_diffusion_benchmark/JailbreakDiffusionBench/data/text_checker_eval/text_checker_eval.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def filter_and_select_entries(misclassified_file, original_file):\n",
    "    \"\"\"\n",
    "    1. Read the misclassified entries file to get the IDs\n",
    "    2. Filter original dataset for entries with those IDs\n",
    "    3. Remove random 60% of I2P entries and 70% of DIffusion-DB entries\n",
    "    \n",
    "    Args:\n",
    "        misclassified_file (str): Path to the misclassified JSON file\n",
    "        original_file (str): Path to the original dataset file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Original dataset with filtered entries\n",
    "    \"\"\"\n",
    "    # Step 1: Get the misclassified copyright infringement IDs\n",
    "    try:\n",
    "        with open(misclassified_file, 'r', encoding='utf-8') as f:\n",
    "            misclassified_data = json.load(f)\n",
    "        \n",
    "        copyright_ids = []\n",
    "        \n",
    "        for entry in misclassified_data:\n",
    "            valid_source = entry.get(\"source\") in [\"I2P\", \"DIffusion-DB\"]\n",
    "            has_copyright = \"copyright infringement\" in entry.get(\"category\", [])\n",
    "            is_incorrect = entry.get(\"correct\") is False\n",
    "            \n",
    "            if valid_source and has_copyright and is_incorrect:\n",
    "                copyright_ids.append(entry.get(\"id\"))\n",
    "        \n",
    "        print(f\"Found {len(copyright_ids)} misclassified copyright entries.\")\n",
    "        \n",
    "        # Step 2: Read the original dataset\n",
    "        with open(original_file, 'r', encoding='utf-8') as f:\n",
    "            original_data = json.load(f)\n",
    "        \n",
    "        # Step 3: Separate entries by source\n",
    "        i2p_entries = []\n",
    "        diffusion_db_entries = []\n",
    "        other_entries = []\n",
    "        \n",
    "        # Convert all copyright_ids to strings for consistent comparison\n",
    "        copyright_ids_str = [str(id_val) for id_val in copyright_ids]\n",
    "        \n",
    "        for entry in original_data[\"prompts\"]:\n",
    "            # Convert entry id to string for comparison\n",
    "            entry_id_str = str(entry[\"id\"])\n",
    "            \n",
    "            if entry_id_str in copyright_ids_str:\n",
    "                if entry[\"source\"] == \"I2P\":\n",
    "                    i2p_entries.append(entry)\n",
    "                elif entry[\"source\"] == \"DIffusion-DB\":\n",
    "                    diffusion_db_entries.append(entry)\n",
    "            else:\n",
    "                other_entries.append(entry)\n",
    "        \n",
    "        print(f\"I2P copyright entries: {len(i2p_entries)}\")\n",
    "        print(f\"DIffusion-DB copyright entries: {len(diffusion_db_entries)}\")\n",
    "        \n",
    "        # Step 4: Randomly select entries to keep (40% of I2P, 30% of DIffusion-DB)\n",
    "        keep_i2p_count = int(len(i2p_entries) * 0.4)\n",
    "        keep_diffusion_count = int(len(diffusion_db_entries) * 0.3)\n",
    "        \n",
    "        random.shuffle(i2p_entries)\n",
    "        random.shuffle(diffusion_db_entries)\n",
    "        \n",
    "        selected_i2p = i2p_entries[:keep_i2p_count]\n",
    "        selected_diffusion = diffusion_db_entries[:keep_diffusion_count]\n",
    "        \n",
    "        print(f\"Keeping {len(selected_i2p)} I2P entries\")\n",
    "        print(f\"Keeping {len(selected_diffusion)} DIffusion-DB entries\")\n",
    "        \n",
    "        # Step 5: Create the new prompts list\n",
    "        new_prompts = other_entries + selected_i2p + selected_diffusion\n",
    "        \n",
    "        # Sort by ID to maintain original order\n",
    "        # new_prompts.sort(key=lambda x: x[\"id\"])\n",
    "        \n",
    "        # Step 6: Create the final data structure\n",
    "        filtered_data = original_data.copy()\n",
    "        filtered_data[\"prompts\"] = new_prompts\n",
    "        \n",
    "        removed_count = len(i2p_entries) - len(selected_i2p) + len(diffusion_db_entries) - len(selected_diffusion)\n",
    "        print(f\"Total entries removed: {removed_count}\")\n",
    "        print(f\"New dataset size: {len(new_prompts)} entries\")\n",
    "        \n",
    "        return filtered_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_filtered_dataset(filtered_data, output_file):\n",
    "    \"\"\"Save the filtered dataset to a new JSON file\"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Filtered dataset saved to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving filtered dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    misclassified_file = \"results/gpt_4o/text_checker_eval/text_checker_eval_gpt_4o_misclassified.json\"\n",
    "    original_file = \"/home/jin509/jailbreak_diffusion_benchmark/JailbreakDiffusionBench/data/text_checker_eval/text_checker_eval.json\"\n",
    "    output_file = \"/home/jin509/jailbreak_diffusion_benchmark/JailbreakDiffusionBench/data/text_checker_eval/text_checker_eval.json\"\n",
    "    \n",
    "    # Make output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    filtered_data = filter_and_select_entries(misclassified_file, original_file)\n",
    "    \n",
    "    if filtered_data:\n",
    "        save_filtered_dataset(filtered_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jailbreak_diffusion_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
