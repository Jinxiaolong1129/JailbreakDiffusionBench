import copy
import os
import random
from typing import TYPE_CHECKING
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np
import torch

from jailbreakbench.classifier import Classifier
from jailbreakbench.defenselib import defense_hparams, perturbations
import jailbreakbench.dataset as dataset

if TYPE_CHECKING:
    from jailbreakbench.llm_wrapper import LLM


class Defense:
    """Base class for jailbreaking defenses."""

    def __init__(self, target_model: "LLM", defense_name: str):
        """Constructor for defense base class.

        Args:
            target_model: Instance of LLMvLLM or LLMLiteLLM.
                Query the target model via target_model.query_llm()
            defense: Name of defense subclass.
        """

        self.target_model = target_model
        self.hparams = defense_hparams.register_hparams(defense_name)

    def query(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]:
        """Query the defense with the supplied input prompt; 
        return response with input and output tokens.
        Subclass should override."""

        raise NotImplementedError()


class SmoothLLM(Defense):
    """SmoothLLM.

    Paper: SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks
    Authors: Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas
    Link: https://arxiv.org/abs/2310.03684
    """

    def __init__(self, target_model: "LLM"):
        super().__init__(target_model=target_model, defense_name="SmoothLLM")

        perturbation_type = self.hparams["perturbation_type"]
        self.perturbation_fn = vars(perturbations)[perturbation_type](q=self.hparams["perturbation_pct"])
        self.judge = Classifier(os.environ["TOGETHER_API_KEY"]).classify_responses

    def query(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]:
        # Perturb the input prompts and obtain responses
        perturbed_prompts = self.perturb_copies(prompt)
        responses = self.target_model.query_llm(perturbed_prompts)

        # Extract prompt/response strings from conversation templates
        user_prompts = [p[1]["content"] for p in perturbed_prompts]

        # Determine whether each perturbed copy is jailbroken
        are_copies_jailbroken = self.judge(user_prompts, responses.responses)

        # Determine whether the majority of responses were jailbroken
        jb_percentage = np.mean(are_copies_jailbroken)
        is_smoothllm_jailbroken = True if jb_percentage > 0.5 else False

        # Randomly select a response consistent with the majority vote
        return random.choice(
            [r for (r, jb) in zip(responses.iterate_elements(), are_copies_jailbroken) if jb == is_smoothllm_jailbroken]
        )

    def perturb_copies(self, prompt):
        """Create perturbed copies of the supplied input prompt."""

        perturbed_prompts = []
        for _ in range(self.hparams["number_of_copies"]):
            prompt_copy = copy.deepcopy(prompt)
            prompt_copy[1]["content"] = self.perturbation_fn(prompt_copy[1]["content"])
            perturbed_prompts.append(prompt_copy)

        return perturbed_prompts


class PerplexityFilter(Defense):
    """Perplexity filter.

    Paper: Baseline Defenses for Adversarial Attacks Against Aligned Language Models
    Authors: Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, 
        Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein
    Link: https://arxiv.org/abs/2309.00614
    """

    REFUSAL_RESPONSE = "I'm sorry, I cannot assist with that request."

    def __init__(self, target_model: "LLM"):
        super(PerplexityFilter, self).__init__(target_model, "PerplexityFilter")
        self.ppl_tokenizer = AutoTokenizer.from_pretrained(
            self.hparams["perplexity_model_path"])
        self.ppl_model = AutoModelForCausalLM.from_pretrained(
            self.hparams["perplexity_model_path"]).cuda()
        self.cn_loss = torch.nn.CrossEntropyLoss(reduction='none')

        goals = dataset.read_dataset().goals
        self.hparams['threshold'] = self.max_log_perplexity(goals)

    def refusal_response(self, prompt):
        """Returns a refusal response."""

        refusal_response = self.target_model.query_llm([prompt])

        # Update the text in the response
        refusal_response.responses[0] = self.REFUSAL_RESPONSE

        # Update the number of response tokens
        num_completion_tokens = len(self.tokenize(self.REFUSAL_RESPONSE, as_pt=False))
        refusal_response.completion_tokens[0] = num_completion_tokens

        return refusal_response[0]

    def tokenize(self, sequence: str, as_pt: bool = True) -> list[int]:
        """Return the input_ids of the input sequence."""

        if not as_pt:
            return self.ppl_tokenizer.encode(sequence)
        return self.ppl_tokenizer.encode(sequence, return_tensors='pt').cuda()
    
    def log_perplexity(self, sequence: str) -> float:
        """Get the log perplexity of a sequence."""

        input_ids = self.tokenize(sequence)
        with torch.no_grad():   
            loss = self.ppl_model(input_ids, labels=input_ids).loss
        return loss.item()

    def max_log_perplexity(self, sequences: list[str]) -> float:
        """Get maximum log perplexity of a sequence."""

        return max([self.log_perplexity(s) for s in sequences])
    
    def log_prob(self, sequence: str):
        """Get the log probabilities of the token."""

        input_ids = self.tokenize(sequence)
        with torch.no_grad():
            logits = self.ppl_model(input_ids, labels=input_ids).logits
        logits = logits[:, :-1, :].contiguous()
        input_ids = input_ids[:, 1:].contiguous()
        log_probs = self.cn_loss(logits.view(-1, logits.size(-1)), input_ids.view(-1))
        return log_probs
    
    def query(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]:
        """Filter prompt based on log perplexity."""

        prompt_string = prompt[1]["content"]

        log_probs = self.log_prob(prompt_string)
        if log_probs.mean() <= self.hparams['threshold']:
            return self.target_model.query_llm([prompt])[0]
        return self.refusal_response(prompt)


DEFENSES: dict[str, type[Defense]] = {
    "SmoothLLM": SmoothLLM,
    "PerplexityFilter": PerplexityFilter
}