import json
import os
from typing import Any, Literal

from jailbreakbench.classifier import Classifier
from jailbreakbench.config import LOGS_PATH, MODEL_NAMES, NUM_BEHAVIORS
from jailbreakbench.llm_together import LLMTogether
from jailbreakbench.llm_wrapper import LLM, LLMOutput

SUBMISSIONS_PATH = "submissions"


def check_file_paths(model_name: str) -> str:
    """Check if the file path for the given model name exists."""
    model_name = model_name.lower()
    if model_name not in MODEL_NAMES:
        raise ValueError(f"Invalid model name: {model_name}")
    file_path = os.path.join(SUBMISSIONS_PATH, f"{model_name}.csv")
    if os.path.exists(file_path):
        raise ValueError(f"{model_name.capitalize()} submission file already exists: {file_path}")
    return file_path

def check_eval_logs_empty():
    nonempty_logs = []
    for model in MODEL_NAMES:
        log_evals_path = os.path.join(LOGS_PATH,"eval",model)
        if os.path.exists(log_evals_path):
            if os.listdir(log_evals_path):
                nonempty_logs.append(log_evals_path)
        else:
            os.makedirs(log_evals_path, exist_ok=True)
    if nonempty_logs:
        raise ValueError(f"Warning, {nonempty_logs} for are not empty. Please clear the logs before running the evaluation.")

def evaluate_prompts(
    prompts_dict: dict[str, dict[str, str]], llm_provider: Literal["vllm"] | Literal["together"] = "together"
) -> dict[str, Any]:
    """
    Query the final responses for each prompt and classify them as jailbroken or not.

    Args:
        prompts_dict: A nested dictionary, model_name -> behavior -> prompt
        llm_provider: The LLM provider to use, either "vllm" or "together"
    Returns:
        A nested dictionary, model_name -> behavior -> (prompts, responses, classifications)
    """
    if len(prompts_dict) != len(MODEL_NAMES):
        raise ValueError("Number of keys in prompts_dict does not match the number of models.")
    if not all(len(v) == NUM_BEHAVIORS for v in prompts_dict.values()):
        raise ValueError("Number of keys in prompts is not equal to NUM_BEHAVIORS.")
    check_eval_logs_empty()
    
    if llm_provider == "vllm":
        try:
            from jailbreakbench.llm_vllm import LLMvLLM
        except ImportError:
            raise ImportError("vllm is not installed. Install it with `pip install vllm`.")
        llm_constructor = lambda model_name: LLMvLLM(model_name=model_name)
    else:
        llm_constructor = lambda model_name: LLMTogether(
            model_name=model_name, together_api_key=os.environ["TOGETHER_API_KEY"]
        )

    evaluation = {}
    for model_name, behaviors_dict in prompts_dict.items():
        sorted_dict = {key: behaviors_dict[key] for key in sorted(behaviors_dict)}
        llm = llm_constructor(model_name=model_name)
        model_dict = generate_full_model_dict(model_name, llm, sorted_dict)
        evaluation[model_name] = model_dict
    return evaluation

def generate_full_model_dict(model_name:str, llm: LLM, behavior_prompt_dict: dict[str: str]):
    none_behaviors = [behavior for behavior, prompt in behavior_prompt_dict.items() if prompt is None]
    valid_behaviors = [behavior for behavior, prompt in behavior_prompt_dict.items() if prompt is not None]
    valid_prompts = [behavior_prompt_dict[beh] for beh in valid_behaviors]
    print(f"Querying responses for {model_name}.")
    outputs = llm._get_responses(valid_prompts)
    responses = outputs.responses

    # Log to evals
    for i in range(len(valid_behaviors)):
        behavior, prompt = valid_behaviors[i], valid_prompts[i]
        cur_output = LLMOutput(responses = [responses[i]], 
                               prompt_tokens=[outputs.prompt_tokens[i]], 
                               completion_tokens=[outputs.completion_tokens[i]])
        llm.log_responses(
            behavior=behavior,
            prompts=[prompt],
            llm_output=cur_output,
            phase="eval",
        )

    print(f"Classifying responses for {model_name}.")
    classifier = Classifier(os.environ["TOGETHER_API_KEY"])

    classifications = classifier.classify_responses(valid_prompts, responses)
    print(f"Number of jailbreaks {model_name}: {sum(classifications)}/{NUM_BEHAVIORS}, (Total of {len(valid_behaviors)} submitted prompts.)")

    model_dict = {}
    for i in range(len(valid_behaviors)):
        model_dict[valid_behaviors[i]] = {
            "prompt": valid_prompts[i],
            "response": responses[i],
            "classification": classifications[i],
        }
    for behavior in none_behaviors:
        model_dict[behavior] = {
            "prompt": None,
            "response": None,
            "classification": False,
        }

    return model_dict

def create_submission(evaluation: dict[str, Any], aggregate_logs: bool = True) -> None:
    """Create a submission file from the evaluation."""
    submission = {}
    path = LOGS_PATH
    if aggregate_logs:
        test_logs = _aggregate_logs(os.path.join(path, "test"))
        eval_logs = _aggregate_logs(os.path.join(path, "eval"))
        submission["summary"] = compute_summary_stats(evaluation, test_logs)
        submission["test_logs"] = test_logs
        submission["eval_logs"] = eval_logs
        
    if not os.path.exists(SUBMISSIONS_PATH):
        os.makedirs(SUBMISSIONS_PATH)
    submission_file = os.path.join(SUBMISSIONS_PATH, "submission.json")
    with open(submission_file, "w") as f:
        json.dump(submission, f, indent = 4)
        print(f"Submission file created at {submission_file}.")
        return submission


def _aggregate_logs(path: str) -> dict[str, Any]:
    """Aggregate logs from the given path."""
    aggregate = {}

    for model_name in os.listdir(path):
        model_path = os.path.join(path, model_name)
        if os.path.isdir(model_path):
            model_dict = {}
            for behavior_file in os.listdir(model_path):
                behavior, ext = os.path.splitext(behavior_file)
                if ext == ".json":
                    model_dict[behavior] = _load_json(os.path.join(model_path, behavior_file))
            aggregate[model_name] = model_dict

    for model_name in MODEL_NAMES:
        if model_name not in aggregate:
            print(f"Warning: No {path} logs found for model {model_name}.")
    return aggregate

def compute_summary_stats(evaluation,test_logs):
    summary_dict = {}
    
    for model_name in MODEL_NAMES:
        total_num_jailbreaks = 0
        num_submitted_prompts = 0
        total_prompt_tokens = 0
        total_response_tokens = 0
        total_queries_used = 0
        summary_dict[model_name] = {"model_summary":{}}
        if model_name in test_logs:
            log_dict = test_logs[model_name]
            behaviors = sorted(list(log_dict.keys()))
            
            for behavior in behaviors:
                summary_behavior_dict = evaluation[model_name][behavior]
                jailbreak_prompt = summary_behavior_dict["prompt"]
                jailbreak_classification = summary_behavior_dict["classification"]
                
                first_jailbreak_query = None
                prompt_tokens = 0
                response_tokens = 0
                queries = 0
                behavior_dict = log_dict[behavior]
                for timestamp, query_logs in behavior_dict:
                    for query in query_logs:
                        prompt_tokens += query["prompt_tokens"]
                        response_tokens += query["response_tokens"]
                        queries += 1    
                        if jailbreak_prompt == query["prompt"] and jailbreak_classification:
                            first_jailbreak_query = queries
                if first_jailbreak_query is None and jailbreak_classification:
                    print(f"Warning: Jailbreak prompt for model {model_name} for behavior {behavior} was not found in the test logs. Prompt: {jailbreak_prompt[:50]}...")
                summary_behavior_dict["number_of_queries_to_jailbreak"] = first_jailbreak_query
                summary_behavior_dict["prompt_tokens_used"] = prompt_tokens
                summary_behavior_dict["response_tokens_used"] = response_tokens
                summary_behavior_dict["queries_used"] = queries

                summary_dict[model_name][behavior] = summary_behavior_dict

                total_num_jailbreaks += jailbreak_classification
                num_submitted_prompts += jailbreak_prompt is not None
                total_prompt_tokens += prompt_tokens
                total_queries_used += queries
                total_response_tokens += response_tokens
            overall_summary_dict = {
                "total_num_jailbreaks": total_num_jailbreaks,
                "num_submitted_prompts": num_submitted_prompts,
                "total_prompt_tokens": total_prompt_tokens,
                "total_response_tokens": total_response_tokens,
                "total_queries_used": total_queries_used,
            }        
            summary_dict[model_name]["model_summary"] = overall_summary_dict
        else:
            print(f"Warning: No test logs found for model {model_name}.")
        
    return summary_dict

def _load_json(path: str) -> Any:
    """Load a JSON file from the given path."""
    with open(path) as f:
        return json.load(f)
