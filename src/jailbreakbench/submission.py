import json
import os
from datetime import datetime
from typing import Any, Literal

import pandas as pd
import pkg_resources

from jailbreakbench.classifier import Classifier
from jailbreakbench.config import LOGS_PATH, MODEL_NAMES, NUM_BEHAVIORS
from jailbreakbench.llm_together import LLMTogether
from jailbreakbench.llm_wrapper import LLM, LLMOutput

SUBMISSIONS_PATH = "submissions"


def check_file_paths(model_name: str) -> str:
    """Check if the file path for the given model name exists."""
    model_name = model_name.lower()
    if model_name not in MODEL_NAMES:
        raise ValueError(f"Invalid model name: {model_name}")
    file_path = os.path.join(SUBMISSIONS_PATH, f"{model_name}.csv")
    if os.path.exists(file_path):
        raise ValueError(f"{model_name.capitalize()} submission file already exists: {file_path}")
    return file_path

def check_eval_logs_empty():
    nonempty_logs = []
    for model in MODEL_NAMES:
        log_evals_path = os.path.join(LOGS_PATH,"eval",model)
        if os.path.exists(log_evals_path):
            if os.listdir(log_evals_path):
                nonempty_logs.append(log_evals_path)
        else:
            os.makedirs(log_evals_path, exist_ok=True)
    if nonempty_logs:
        raise ValueError(f"Warning, {nonempty_logs} are not empty. Please delete them before running the evaluation.")

def evaluate_prompts(
    prompts_dict: dict[str, dict[str, str]], llm_provider: Literal["vllm"] | Literal["together"] = "together"
) -> dict[str, Any]:
    """
    Query the final responses for each prompt and classify them as jailbroken or not.

    Args:
        prompts_dict: A nested dictionary, model_name -> behavior -> prompt
        llm_provider: The LLM provider to use, either "vllm" or "together"
    Returns:
        A nested dictionary, model_name -> behavior -> (prompts, responses, classifications)
    """
    if len(prompts_dict) != len(MODEL_NAMES):
        raise ValueError("Number of keys in prompts_dict does not match the number of models.")
    if not all(len(v) == NUM_BEHAVIORS for v in prompts_dict.values()):
        raise ValueError("Number of keys in prompts is not equal to NUM_BEHAVIORS.")
    check_eval_logs_empty()
    
    if llm_provider == "vllm":
        try:
            from jailbreakbench.llm_vllm import LLMvLLM
        except ImportError:
            raise ImportError("vllm is not installed. Install it with `pip install vllm`.")
        llm_constructor = lambda model_name: LLMvLLM(model_name=model_name)
    elif llm_provider == "together":
        llm_constructor = lambda model_name: LLMTogether(
            model_name=model_name, together_api_key=os.environ["TOGETHER_API_KEY"]
        )
    else:
        raise ValueError(f"Invalid llm_provider: {llm_provider}. Should be one of 'vllm' or 'together'.")

    evaluation = {}
    for model_name, behaviors_dict in prompts_dict.items():
        sorted_dict = {key: behaviors_dict[key] for key in sorted(behaviors_dict)}
        llm = llm_constructor(model_name=model_name)
        model_dict = generate_full_model_dict(model_name, llm, sorted_dict)
        evaluation[model_name] = model_dict
    return evaluation

def generate_full_model_dict(model_name:str, llm: LLM, behavior_prompt_dict: dict[str: str]):
    none_behaviors = [behavior for behavior, prompt in behavior_prompt_dict.items() if prompt is None]
    valid_behaviors = [behavior for behavior, prompt in behavior_prompt_dict.items() if prompt is not None]
    valid_prompts = [behavior_prompt_dict[beh] for beh in valid_behaviors]
    print(f"Querying responses for {model_name}.")
    outputs = llm._get_responses(valid_prompts)
    responses = outputs.responses

    # Log to evals
    for i in range(len(valid_behaviors)):
        behavior, prompt = valid_behaviors[i], valid_prompts[i]
        cur_output = LLMOutput(responses = [responses[i]], 
                               prompt_tokens=[outputs.prompt_tokens[i]], 
                               completion_tokens=[outputs.completion_tokens[i]])
        llm.log_responses(
            behavior=behavior,
            prompts=[prompt],
            llm_output=cur_output,
            phase="eval",
        )

    print(f"Classifying responses for {model_name}.")
    classifier = Classifier(os.environ["TOGETHER_API_KEY"])

    classifications = classifier.classify_responses(valid_prompts, responses)
    print(f"Number of jailbreaks {model_name}: {sum(classifications)}/{NUM_BEHAVIORS}, (Total of {len(valid_behaviors)} submitted prompts.)")

    model_dict = {}
    for i in range(len(valid_behaviors)):
        model_dict[valid_behaviors[i]] = {
            "Prompt": valid_prompts[i],
            "Response": responses[i],
            "Jailbroken": classifications[i],
        }
    for behavior in none_behaviors:
        model_dict[behavior] = {
            "Prompt": None,
            "Response": None,
            "Jailbroken": False,
        }

    return model_dict

def create_submission(evaluation: dict[str, Any], 
                      method_name: str,
                      attack_type: str,
                      method_params: dict = None) -> None:
    """Create a submission file from the evaluation."""
    # Construct the path to data.csv within the package
    submission = {}
    path = LOGS_PATH

    test_logs = _aggregate_logs(os.path.join(path, "test"))
    eval_logs = _aggregate_logs(os.path.join(path, "eval"))
    submission["Summaries"] = compute_summary_stats(evaluation, test_logs)
    submission["test_logs"] = test_logs
    submission["eval_logs"] = eval_logs
        
    if not os.path.exists(SUBMISSIONS_PATH):
        os.makedirs(SUBMISSIONS_PATH)
    submission_file = os.path.join(SUBMISSIONS_PATH, "submission.json")
    with open(submission_file, "w") as f:
        json.dump(submission, f, indent = 4)
        print(f"Submission file created at {submission_file}.")

    create_artifacts(submission["Summaries"], method_name, attack_type, method_params)
    return submission

def create_artifacts(model_summaries, method_name, attack_type, method_params):
    for model_name in model_summaries:
        model_summary = {}
        summary_dict = {"Method":method_name,
                        "Model Name": model_name,
                        "Attack Type":attack_type,
                        }
        summary_dict.update(model_summaries[model_name]["Model Summary"])
        
        # Current time in ISO 8601 format
        timestamp = datetime.utcnow().isoformat() + "Z"  
        summary_dict["Evaluation Date"] = timestamp
        summary_dict["Method Params"] = method_params

        model_summary["Parameters"] = summary_dict
        model_summary["Jailbreaks"] = model_summaries[model_name]["Jailbreaks"]
        model_name_underscore = model_name.replace("-", "_")
        model_name_underscore = model_name_underscore.replace(" ", "_")
        file_path = os.path.join(SUBMISSIONS_PATH, f"{model_name_underscore}_jailbreaks.json")
        with open(file_path, "w") as f:
            json.dump(model_summary, f, indent = 4)
            print(f"Jailbreak artifact for {model_name} created at {file_path}.")
        

def _aggregate_logs(path: str) -> dict[str, Any]:
    """Aggregate logs from the given path."""
    aggregate = {}
    if os.path.isdir(path):
        for model_name in os.listdir(path):
            if model_name in MODEL_NAMES:
                model_log_path = os.path.join(path, model_name)
                model_dict = {}
                for behavior_file in os.listdir(model_log_path):
                    behavior, ext = os.path.splitext(behavior_file)
                    if ext == ".json":
                        model_dict[behavior] = _load_json(os.path.join(model_log_path, behavior_file))
                aggregate[model_name] = model_dict

    for model_name in MODEL_NAMES:
        if model_name not in aggregate:
            print(f"Warning: No {path} logs found for model {model_name}.")
    return aggregate

def compute_summary_stats(evaluation,test_logs):
    summary_dict = {}
    for model_name in MODEL_NAMES:
        if model_name in test_logs:
            model_test_logs = test_logs[model_name]
        else:
            model_test_logs = {}
        summary_dict[model_name] = _compute_model_summary(model_name, evaluation[model_name], model_test_logs)
    return summary_dict

def _compute_model_summary(model_name, model_eval, log_dict):
    data_path = pkg_resources.resource_filename('jailbreakbench', 'data/behaviors.csv')
    behaviors_df = pd.read_csv(data_path)

    total_num_jailbreaks = 0
    num_submitted_prompts = 0
    total_prompt_tokens = 0
    total_response_tokens = 0
    total_queries_used = 0
    
    output = {}
    jailbreaks_list = []
    # Iterate through each behavior in the benchmark
    for row in behaviors_df.iterrows():
        row_data = row[1]
        index = row_data["Index"]
        goal = row_data["Goal"]
        behavior = row_data["Identifier"]
        category = row_data["Category"]

        if behavior not in model_eval:
            raise ValueError(f"Behavior {behavior} not found in evaluation for model {model_name}.")
        
        eval_behavior_dict = model_eval[behavior]
        jb_dict = {}
        jailbreak_prompt = eval_behavior_dict["Prompt"]
        response = eval_behavior_dict["Response"]
        classification = eval_behavior_dict["Jailbroken"]
        
        num_submitted_prompts += jailbreak_prompt is not None
        total_num_jailbreaks += classification

        prompt_tokens = 0
        response_tokens = 0
        queries = 0
        first_jailbreak_query = None
        # Parse logs for prompt tokens, response tokens, and queries to jailbreak
        if log_dict:
            first_jailbreak_query = None
            log_behavior_dict = log_dict[behavior]
            for timestamp, query_logs in log_behavior_dict:
                for query in query_logs:
                    prompt_tokens += query["prompt_tokens"]
                    response_tokens += query["response_tokens"]
                    queries += 1    
                    if jailbreak_prompt == query["prompt"] and classification:
                        first_jailbreak_query = queries
            if first_jailbreak_query is None and classification:
                print(f"Warning: Jailbreak prompt for model {model_name} for behavior {behavior} was not found in the test logs. Prompt: {jailbreak_prompt[:50]}...")

            total_prompt_tokens += prompt_tokens
            total_queries_used += queries
            total_response_tokens += response_tokens


        jb_dict = {
                    "Index": index,
                    "Goal": goal, 
                    "Category": category, 
                    "Identifier": behavior,
                    "Prompt": jailbreak_prompt,
                    "Response": response,
                    "Jailbroken": classification,
                    "Queries to Jailbreak": first_jailbreak_query, 
                    "Total Queries": queries, 
                    "Prompt Tokens": prompt_tokens, 
                    "Response Tokens": response_tokens,
                    }
        
        jb_dict = map_zero_to_none(jb_dict)
        jailbreaks_list.append(jb_dict)


    model_summary_dict = {
        "Total Number of Jailbreaks": total_num_jailbreaks,
        "Attack Success Rate": total_num_jailbreaks / NUM_BEHAVIORS,
        "Number of Submitted Prompts": num_submitted_prompts,
        "Total Prompt Tokens": total_prompt_tokens,
        "Total Response Tokens": total_response_tokens,
        "Total Queries": total_queries_used,
    }        
    model_summary_dict = map_zero_to_none(model_summary_dict)

    output["Model Summary"] = model_summary_dict
    output["Jailbreaks"] = jailbreaks_list      
    return output

def map_zero_to_none(d: dict[str, Any]) -> dict[str, Any]:
    """Map zero values to None in the given dictionary."""
    return {k: (None if v == 0 and k != "Index" and type(v) is int else v) for k, v in d.items()}

def _load_json(path: str) -> Any:
    """Load a JSON file from the given path."""
    with open(path) as f:
        return json.load(f)
