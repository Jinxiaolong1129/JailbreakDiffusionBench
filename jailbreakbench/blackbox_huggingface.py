import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gc
from typing import  List
from jailbreakbench.blackbox_wrapper import BlackBoxLLM
from jailbreakbench.config import TEMPERATURE, TOP_P, MAX_GENERATION_LENGTH, MODEL_NAMES, MODEL_PATHS


class BlackBoxHuggingFace(BlackBoxLLM):
    def __init__(self, model_name: str):
        """Initializes the BlackBoxHuggingFace with the specified model name."""
        if self.model_name not in MODEL_NAMES:
            raise ValueError(f"Invalid model name: {model_name}")
        super().__init__(model_name)
        self._load_model()
        
        
    def _load_model(self):  
        """Loads the model and tokenizer based on the model name."""
        model_path = MODEL_PATHS.get("vicuna" if "vicuna" in self.model_name else "llama-2")

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,device_map="auto"
        ).eval()

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            use_fast=False
        )
        
        self._update_tokenizer()
        self.temperature = TEMPERATURE
        self.top_p = TOP_P
        self.max_n_tokens = MAX_GENERATION_LENGTH
    
    def _update_tokenizer(self):
        # Maybe the model name is in model.config double check
        if 'llama' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.unk_token
            self.tokenizer.padding_side = 'left'
        if 'vicuna' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.padding_side = 'left'
        if not self.tokenizer.pad_token:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def _get_responses(self, prompts_list: List[str]) -> List[str]:

        """Generates responses from the model for the given prompts."""
        inputs = self.tokenizer(prompts_list, return_tensors='pt', padding=True)
        inputs = {k: v.to(self.model.device.index) for k, v in inputs.items()}
    
        # Batch generation
        do_sample = self.temperature > 0

        output_ids = self.model.generate(
            **inputs,
            max_new_tokens=self.max_n_tokens, 
            do_sample=do_sample,
            eos_token_id=self.eos_token_ids,
            top_p=self.top_p,
            temperature=self.temperature
        )
            
        # If the model is not an encoder-decoder type, slice off the input tokens
        if not self.model.config.is_encoder_decoder:
            output_ids = output_ids[:, inputs["input_ids"].shape[1]:]

        # Batch decoding
        outputs_list = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        for key in inputs:
            inputs[key].to('cpu')
        output_ids.to('cpu')
        del inputs, output_ids
        gc.collect()
        torch.cuda.empty_cache()

        return outputs_list

   

          