import os 
import json
from blackbox_llm import BlackBoxLLM
from config import MODEL_NAMES, CLASSIFIER_PATH, NUM_BEHAVIORS, NUM_JAILBREAKS_PER_BEHAVIOR
from classifier import Classifier
    
        
        

def check_file_paths(model_name):
    model_name = model_name.lower()
    assert model_name in MODEL_NAMES
    file_path = f'submissions/{model_name}.csv'
    if os.path.exists(file_path):
        raise ValueError(f"{model_name.capitalize()} submission file already exists: {file_path}")
    return file_path

def evaluate_prompts(prompts_dict):
    """
    Query the final responses for each prompt and classify them as jailbroken or not.

    Args:
        prompts_dict: A nested dictionary, model_name -> behavior -> prompt
    Returns:

    """
    # Check that the number of keys in prompts_dict is equal to the number of models
    assert len(prompts_dict) == len(MODEL_NAMES)
    # Check the number of keys in prompts is equal to NUM_BEHAVIORS
    assert all(len(v) == NUM_BEHAVIORS for v in prompts.values())
    classifier = Classifier(CLASSIFIER_PATH)

    evaluation = {}
    for model_name, behaviors_dict in prompts_dict.items():
        print(f"Querying responses for {model_name} and classifying jailbreaks.")
        model_dict = {}
        llm = BlackBoxLLM(model_name=model_name)
        
        for behavior, prompts in behaviors_dict.items():
            responses = llm.query(prompts, behavior, phase="eval")
            jailbroken_classification = classifier.classify_responses(prompts, responses)
            model_dict[behavior] = {"prompts": prompts, "responses": responses, "classifications": jailbroken_classification}
        evaluation[model_name] = model_dict

    return evaluation

def create_submission(evaluation, aggregate_logs=True):
    if aggregate_logs:
        test_logs = _aggregate_logs("jailbreakbench/logs/test")
        eval_logs = _aggregate_logs("jailbreakbench/logs/eval")
        evaluation["test_logs"] = test_logs
        evaluation["eval_logs"] = eval_logs
    # Save evaluation to a json file
    with open("submissions/submission.json", "w") as f:
        json.dump(evaluation, f)

def _aggregate_logs(path):
    
    aggregate = {}

    for model_name in os.listdir(path):
        model_path = os.path.join(path, model_name)
        if os.path.isdir(model_path):
            model_dict = {}
            for behavior_file in os.listdir(model_path):
                behavior, ext = os.path.splitext(behavior_file)
                if ext == ".json":
                    with open(os.path.join(model_path, behavior_file), "r") as f:
                        model_dict[behavior] = json.load(f)
            aggregate[model_name] = model_dict

    return aggregate

