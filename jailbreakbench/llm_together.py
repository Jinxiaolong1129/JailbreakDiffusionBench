import litellm

from jailbreakbench.config import MODEL_NAMES, SYSTEM_PROMPTS, TOGETHER_MODEL_NAMES, Model
from jailbreakbench.llm_wrapper import LLM


class LLMTogether(LLM):
    _together_model_name: str
    _together_api_key: str
    system_prompt: str

    def __init__(self, model_name: str, together_api_key: str):
        super().__init__(model_name)
        """Initializes the LLMTogether with the specified model name."""
        if self.model_name not in MODEL_NAMES:
            raise ValueError(f"Invalid model name: {model_name}")
        self.model_enum = Model(self.model_name)
        self._initialize_model(together_api_key)
        if self.temperature == 0:
            self.generation_params = {"temperature": 0}
        else:
            self.generation_params = {"top_p": 0.9}
        self.generation_params["max_tokens"] = self.max_n_tokens

    def _initialize_model(self, together_api_key: str):
        self._convert_name(self.model_name)
        self._together_api_key = together_api_key
        self.system_prompt = SYSTEM_PROMPTS[self.model_enum]

    def _convert_name(self, model_name: str):
        self._together_model_name = TOGETHER_MODEL_NAMES[Model(model_name)]

    def _build_llm_input(self, prompt: str) -> list[dict[str, str]]:
        chat = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt},
        ]
        return chat

    def _update_prompt_template(self):
        if self.model_name == "vicuna":
            litellm.register_prompt_template(
                model=self._together_model_name,
                roles={
                    "system": {"pre_message": "", "post_message": "\n"},
                    "user": {"pre_message": "\nUSER: ", "post_message": "\nASSISTANT: "},
                    "assistant": {
                        "pre_message": "",
                        "post_message": "</s>",
                    },
                },
            )
        elif self.model_name == "llama-2":
            litellm.register_prompt_template(
                model=self._together_model_name,
                roles={
                    "system": {"pre_message": "<s>[INST] <<SYS>>\n", "post_message": "\n<</SYS>>\n\n"},
                    "user": {"pre_message": "", "post_message": " [/INST]"},
                    "assistant": {"pre_message": "", "post_message": " </s><s>"},
                },
            )

    def _get_responses(self, prompts: list[str], max_new_tokens: int = None) -> list[str]:
        if max_new_tokens is None:
            max_new_tokens = self.max_n_tokens
        # Need to update prompt template in case it was modified somewhere else
        self._update_prompt_template()
        llm_inputs = [self._build_llm_input(prompt) for prompt in prompts]
        outputs = litellm.batch_completion(
            model=self._together_model_name,
            messages=llm_inputs,
            api_key=self._together_api_key,
            stop=["</s>", "[/INST]"],
            **self.generation_params,
        )
        responses = [output["choices"][0]["message"].content for output in outputs]
        if Model(self.model_name) is Model.llama_2:
            responses = [
                response[2:] for response in responses
            ]  # Trim first space generated when querying together.ai through litellm
        return responses
