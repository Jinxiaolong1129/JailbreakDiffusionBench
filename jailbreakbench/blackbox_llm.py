import os
import json
import threading
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gc
from typing import Dict, List
from pathlib import Path
import datetime

from config import TEMPERATURE, TOP_P, MAX_GENERATION_LENGTH, MODEL_NAMES, MODEL_PATHS


class BlackBoxLLM:
    def __init__(self, model_name):
        self.model_name = model_name.lower()
        if self.model_name not in MODEL_NAMES:
            raise ValueError(f"Invalid model name: {model_name}")
        
        self._load_model()
        self.lock = threading.Lock()
        if not os.path.exists('logs'):
            os.makedirs('logs')
        
    def _load_model(self):  
        if "vicuna" in self.model_name:
            model_path = MODEL_PATHS["vicuna"]
        elif "llama" in self.model_name:
            model_path = MODEL_PATHS["llama-2"]

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,device_map="auto"
        ).eval()

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            use_fast=False
        )
        
        self._update_tokenizer()
        self.temperature = TEMPERATURE
        self.top_p = TOP_P
        self.max_n_tokens = MAX_GENERATION_LENGTH
    


    def log_responses(self, behavior, prompts, responses, phase):
        
        file_path = f'logs/{phase}/{self.model_name}/{behavior}.json'
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)

        with self.lock:
            if not os.path.exists(file_path):
                with open(file_path, 'w') as file:
                    file.write('[]')
            
            with open(file_path, 'r+') as file:
                file.seek(0, os.SEEK_END)
                position = file.tell() - 1
                if position < 0:
                    position = 0
                file.seek(position, os.SEEK_SET)
                print(position)
                if position > 1:
                    file.write(',')

                timestamp =  datetime.datetime.now(datetime.timezone.utc).isoformat()
                log_entries = [{'prompt': p, 'response': r} for p, r in zip(prompts, responses)]
                log_entries.append({'timestamp': timestamp})
                
                json.dump(log_entries, file)
                file.write(']')

  
    def query(self, prompts: List, behavior: str, max_n_tokens = None, phase = "dev"):
        assert phase in ["dev", "test", "eval"]
        if max_n_tokens is None:
            max_n_tokens = self.max_n_tokens
        elif max_n_tokens > self.max_n_tokens:
            raise ValueError(f"max_n_tokens must be less than or equal to {self.max_n_tokens}.")
        
        responses = self._get_responses(prompts, max_n_tokens)
        
        self.log_responses(behavior,prompts,responses,phase)
        return responses

  
    def _get_responses(self, prompts_list: List, max_n_tokens):
        inputs = self.tokenizer(prompts_list, return_tensors='pt', padding=True)
        inputs = {k: v.to(self.model.device.index) for k, v in inputs.items()}
    
        # Batch generation
        do_sample = self.temperature > 0


        output_ids = self.model.generate(
            **inputs,
            max_new_tokens=max_n_tokens, 
            do_sample=do_sample,
            eos_token_id=self.eos_token_ids,
            top_p=self.top_p,
            temperature=self.temperature
        )
            
        # If the model is not an encoder-decoder type, slice off the input tokens
        if not self.model.config.is_encoder_decoder:
            output_ids = output_ids[:, inputs["input_ids"].shape[1]:]

        # Batch decoding
        outputs_list = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        for key in inputs:
            inputs[key].to('cpu')
        output_ids.to('cpu')
        del inputs, output_ids
        gc.collect()
        torch.cuda.empty_cache()

        return outputs_list
     

    def _update_tokenizer(self):
        # Maybe the model name is in model.config double check
        if 'llama' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.unk_token
            self.tokenizer.padding_side = 'left'
        if 'vicuna' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.padding_side = 'left'
        if not self.tokenizer.pad_token:
            self.tokenizer.pad_token = self.tokenizer.eos_token


    