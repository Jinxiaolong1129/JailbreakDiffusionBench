import os
import json
import threading
from typing import List
from pathlib import Path
import datetime

class LLM:
    def __init__(self, model_name: str):
        """Initializes the LLM with the specified model name."""
        self.model_name = model_name.lower()
        
        self.lock = threading.Lock()
        if not os.path.exists('logs'):
            os.makedirs('logs')

    def query(self, prompts: List[str],  behavior: str = "unspecified_behavior", phase: str = "dev", max_new_tokens: int = None) -> List[str]:
        """Queries the model with the given prompts and logs the responses."""
        if phase not in ["dev", "test", "eval"]:
            raise ValueError(f"Invalid phase: {phase}. Expected one of: 'dev', 'test', 'eval'")

        responses = self._get_responses(prompts, max_new_tokens)
        self.log_responses(behavior,
                           prompts,
                           responses,
                           phase)
        return responses
    
    def _get_responses(self, prompts_list: List[str], max_new_tokens: int) -> List[str]:
        raise NotImplementedError("Subclasses must implement this method.")
    
    def log_responses(self, behavior: str, prompts: List[str], responses: List[str], phase: str):
        """Logs the responses to a json file."""
        file_path = f'jailbreakbench/logs/{phase}/{self.model_name}/{behavior}.json'
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)

        with self.lock:
            if not os.path.exists(file_path):
                with open(file_path, 'w') as file:
                    file.write('[]')
            
            with open(file_path, 'r+') as file:
                file.seek(0, os.SEEK_END)
                position = file.tell() - 1
                if position < 0:
                    position = 0
                file.seek(position, os.SEEK_SET)
                
                if position > 1:
                    file.write(',')

                timestamp =  datetime.datetime.now(datetime.timezone.utc).isoformat()
                log_entries = [{'prompt': p, 'response': r} for p, r in zip(prompts, responses)]
                log_entries.append({'timestamp': timestamp})
                
                json.dump(log_entries, file)
                file.write(']')

class LLMDummy(LLM):
    """Dummy class for testing without GPU"""
    def _get_responses(self, prompts_list: List[str], max_new_tokens: int) -> List[str]:
        """Generates dummy responses for the given prompts."""
        outputs = [f"Response to `{prompt}`" for prompt in prompts_list]
        return outputs
