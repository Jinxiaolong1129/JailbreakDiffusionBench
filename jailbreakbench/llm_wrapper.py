import datetime
import json
import os
import threading
from pathlib import Path

from jailbreakbench.config import MAX_GENERATION_LENGTH, SYSTEM_PROMPTS, TEMPERATURE, TOP_P, Model

class LLMResponse:
    def __init__(self, responses, prompt_tokens, completion_tokens):
        # LLM responses for a batch.
        # Initialize each list. If no list is provided, initialize to an empty list.
        self.data = {
            "responses": responses if responses is not None else [],
            "prompt_tokens": prompt_tokens if prompt_tokens is not None else [],
            "completion_tokens": completion_tokens if completion_tokens is not None else [],
        }
    
    def __getitem__(self, key):
        return self.data[key]
    
    def __setitem__(self, key, value):
        if not isinstance(value, list):
            raise ValueError(f"Value for {key} must be a list.")
        self.data[key] = value
    
    def __repr__(self):
        return f"LLMResponse(responses={self.data['responses']}, prompt_tokens={self.data['prompt_tokens']}, completion_tokens={self.data['completion_tokens']})"


class LLM:
    temperature: float
    top_p: float
    max_n_tokens: int

    def __init__(self, model_name: str):
        """Initializes the LLM with the specified model name."""
        self.model_name = Model(model_name.lower())
        self.system_prompt = SYSTEM_PROMPTS[self.model_name]
        self.temperature = TEMPERATURE
        self.top_p = TOP_P
        self.max_n_tokens = MAX_GENERATION_LENGTH
        self.lock = threading.Lock()
        if not os.path.exists("logs"):
            os.makedirs("logs")

    def query(
        self, prompts: list[str], 
        behavior: str = "unspecified_behavior", 
        phase: str = "dev",
        max_new_tokens: int = None
    ) -> LLMResponse:
        """Queries the model with the given prompts and logs the responses. Returns the responses and token counts."""
        if phase not in ["dev", "test", "eval"]:
            raise ValueError(f"Invalid phase: {phase}. Expected one of: 'dev', 'test', 'eval'")

        llm_response = self._get_responses(prompts, max_new_tokens)
        self.log_responses(behavior, prompts, llm_response, phase, )
        return llm_response

    def _get_responses(self, prompts_list: list[str], max_new_tokens: int | None) -> LLMResponse:
        raise NotImplementedError("Subclasses must implement this method.")

    def log_responses(self, behavior: str, prompts: list[str], llm_response:dict[str, list[str] | list[int]], phase: str) -> None:
        """Logs the responses to a json file."""
        file_path = f"logs/{phase}/{self.model_name}/{behavior}.json"
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        prompt_tokens = llm_response["prompt_tokens"]
        completion_tokens = llm_response["completion_tokens"]
        responses = llm_response["responses"]
        with self.lock:
            if not os.path.exists(file_path):
                with open(file_path, "w") as file:
                    json.dump([], file, indent=4)

            with open(file_path, "r+") as file:
                file.seek(0, os.SEEK_END)
                position = file.tell() - 1
                if position < 0:
                    position = 0
                file.seek(position, os.SEEK_SET)

                if position > 1:
                    file.write(",\n\t")

                timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()
                log_entries = [{"timestamp": timestamp}]
                log_entries.append([{"prompt": p, 
                "response": r, "prompt_tokens": p_tokens, "response_tokens": r_tokens} for p, r, p_tokens, r_tokens in zip(prompts, responses, prompt_tokens, completion_tokens)])

                json.dump(log_entries, file, indent=4, separators=(',', ': '))
        
                # Close the list
                file.write("\n]")




class LLMDummy(LLM):
    """Dummy class for testing without GPU"""

    def _get_responses(self, prompts_list: list[str], max_new_tokens: int) -> LLMResponse:
        """Generates dummy responses for the given prompts."""
        outputs = [f"Response to `{prompt}`" for prompt in prompts_list]
        llm_response = LLMResponse(outputs, [len(prompt) for prompt in prompts_list], [len(output) for output in outputs])
        
        return llm_response
