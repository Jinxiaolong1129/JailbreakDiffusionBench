import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gc
from typing import  List
from jailbreakbench.llm_wrapper import LLM
from jailbreakbench.config import TEMPERATURE, TOP_P, MAX_GENERATION_LENGTH, MODEL_NAMES, MODEL_PATHS
from fastchat.model import get_conversation_template

class LLMHuggingFace(LLM):
    def __init__(self, model_name: str, preloaded_model: object = None):
        super().__init__(model_name)
        """Initializes the LLMHuggingFace with the specified model name."""
        if self.model_name not in MODEL_NAMES:
            raise ValueError(f"Invalid model name: {model_name}")
        if preloaded_model is None:
            self._load_model()
        else:
            self.model = preloaded_model

        self._initialize_model()
        
    
    def _load_model(self):  
        """Loads the model and tokenizer based on the model name."""
        
        model_path = MODEL_PATHS[self.model_name]
        try: 
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,device_map="auto"
            ).eval()
        except EnvironmentError as e:
            print(f"Error loading model. Did you update the model path in jailbreakbench/config.py?")
            raise e
        except Exception as e:
            print(e)

    def _initialize_model(self):
        model_path = MODEL_PATHS[self.model_name]
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            use_fast=False
        )
        
        self._update_tokenizer()
        self.temperature = TEMPERATURE
        self.top_p = TOP_P
        self.max_n_tokens = MAX_GENERATION_LENGTH
    
    def _update_tokenizer(self):
        if 'llama' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.unk_token
            self.tokenizer.padding_side = 'left'
        if 'vicuna' in self.model_name:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.padding_side = 'left'
        if not self.tokenizer.pad_token:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def _get_responses(self, prompts_list: List[str]) -> List[str]:
        """Generates responses from the model for the given prompts."""
        full_prompt_list = self._prompt_to_conv(prompts_list)
        inputs = self.tokenizer(full_prompt_list, return_tensors='pt', padding=True)
        inputs = {k: v.to(self.model.device.index) for k, v in inputs.items()}
    
        # Batch generation
        do_sample = self.temperature > 0

        output_ids = self.model.generate(
            **inputs,
            max_new_tokens=self.max_n_tokens, 
            do_sample=do_sample,
            eos_token_id=self.tokenizer.eos_token_id,
            top_p=self.top_p,
            temperature=self.temperature
        )
            
        # If the model is not an encoder-decoder type, slice off the input tokens
        if not self.model.config.is_encoder_decoder:
            output_ids = output_ids[:, inputs["input_ids"].shape[1]:]

        # Batch decoding
        outputs_list = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        for key in inputs:
            inputs[key].to('cpu')
        output_ids.to('cpu')
        del inputs, output_ids
        gc.collect()
        torch.cuda.empty_cache()

        return outputs_list

    def _prompt_to_conv(self, prompts_list):
        batchsize = len(prompts_list)
        convs_list = [self._conv_template(self.model_name) for _ in range(batchsize)]
        full_prompts = []
        for conv, prompt in zip(convs_list, prompts_list):
            conv.append_message(conv.roles[0], prompt)
            conv.append_message(conv.roles[1], None) 
            full_prompt = conv.get_prompt()
            # Need this to avoid extraneous space in generation
            if 'llama-2' in self.model_name:
                full_prompt += " "
            full_prompts.append(full_prompt)
        return full_prompts

    def _conv_template(self,template_name):
        template = get_conversation_template(template_name)
        return template