import json
import jailbreakbench as jbb

# Read in the behaviors from the JailbreakBench dataset
dataset = jbb.read_dataset()
behaviors = dataset.behaviors

# Load Vicuna and Llama2 jailbreak strings
with open("prompts/vicuna.json", 'r') as f:
    vicuna_prompts = json.load(f)
with open("prompts/llama2.json", 'r') as f:
    llama2_prompts = json.load(f)

# Create a dictionary containing the JailbreakBench behaviors
# and the corresponding jailbreak strings for Vicuna and Llama2
all_prompts = {
    "vicuna-13b-v1.5": {b: s for b, s in zip(behaviors, vicuna_prompts)}, 
    "llama-2-7b-chat-hf" : {b: s for b, s in zip(behaviors, llama2_prompts)}
}

# Evaluate the performance of your jailbreak strings against the 
# JailbreakBench classifier.  Note that for this step to work, you need
# to export your together.ai API key as an environment variable, e.g.,
#
# export TOGETHER_API_KEY="<your-together-ai-api-key>"
#
# All of the relevant output will be saved to a folder called `logs/`
# in your current working directory.  
# Note 1: If you'd like to run this step locally, set `llm_provider="vllm"`. 
# Note 2: If you'd like to run a defense, set the `defense` flag.  For
# example, to run the SmoothLLM defense, set `defense="SmoothLLM`.
evaluation = jbb.evaluate_prompts(all_prompts, llm_provider="litellm")

# Create a dictionary containing the parameters of your attack
method_params = {
    "n-iter": 500,
    "batch-size": 512,
    "judge-model": "jailbreakbench",
    "target-max-n-tokens": 150,
    "attack-max-n-tokens": 300
}

# Create a folder which will comprise your submission to JailbreakBench.
# The output of this step will be stored in a folder called `submissions`
# in your current working directory.  To submit to our leaderboard, please
# zip this file and submit it <WHERE?>.
jbb.create_submission(
    evaluation, 
    method_name="GCG", 
    attack_type="white_box", 
    method_params=method_params
)